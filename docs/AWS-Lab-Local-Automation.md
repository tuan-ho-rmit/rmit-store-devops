# RMIT Store — Local-Driven AWS Setup (Lab-Friendly)

One-command, reproducible provisioning from your laptop: CloudFormation (VPC + 5 EC2), Ansible bootstrap (Docker, k3s, Mongo, Jenkins), Jenkins auto-config (JCasC), and hooks for Helm deploys. Designed to work with limited AWS lab roles (no IAM roles, no EKS, no ALB). Uses t3.micro everywhere.

## 0) Prerequisites (on your laptop)

AWS CLI configured to your lab account (aws configure)

Tools: jq, ansible (or pip install ansible), kubectl, helm, docker, ssh

Docker Hub account (username + PAT)

GitHub repo containing your app (with Jenkinsfile and helm/rmit-store chart)

aws --version
jq --version
ansible --version
kubectl version --client
helm version
docker --version

## 1) Directory Layout
```text
rmit-aws/
├─ Makefile
├─ .env.example
├─ infra/
│  ├─ 00-network.yaml
│  └─ 01-compute.yaml
├─ ansible/
│  ├─ inventory.ini            # auto-generated by Makefile
│  ├─ group_vars/
│  │  └─ all.yaml             # auto-generated by Makefile
│  ├─ site.yaml
│  └─ roles/
│     ├─ common/tasks/main.yaml
│     ├─ mongo/tasks/main.yaml
│     ├─ k3s_master/tasks/main.yaml
│     ├─ k3s_worker/tasks/main.yaml
│     └─ jenkins/tasks/main.yaml
└─ jenkins/
   ├─ plugins.txt
   └─ casc.yaml.j2             # JCasC template (Ansible fills in)
```

## 2) Environment File

Create .env from the template below (edit values for your lab):

```env
# .env.example  (copy to .env and fill)
AWS_REGION=ap-southeast-2

# Use an EXISTING lab key pair name (lab roles often disallow create-key-pair)
EC2_KEY_NAME=lab-existing-keypair-name

# t3.micro to stay under budget
TYPE_JENKINS=t3.micro
TYPE_MASTER=t3.micro
TYPE_WORKER=t3.micro
TYPE_MONGO=t3.micro

# Docker Hub
DOCKERHUB_USER=your_dockerhub_user
DOCKERHUB_PAT=your_dockerhub_pat

# GitHub (private repo requires a PAT; polling used so webhooks are optional)
GITHUB_OWNER=your_github_user_or_org
GITHUB_REPO=your_repo_name
GITHUB_TOKEN=ghp_xxx_with_repo_scope

# Jenkins admin bootstrap (used by JCasC only for defaults)
JENKINS_URL_HINT=http://localhost:8080/

# Optional: Ubuntu AMI ID fallback (if SSM GetParameter is blocked in your lab)
# Leave empty to use SSM; otherwise set a region-matching Ubuntu 22.04 AMI.
UBUNTU_AMI_OVERRIDE=
```

## 3) CloudFormation — Network (public VPC + SGs)

infra/00-network.yaml
```yaml

AWSTemplateFormatVersion: '2010-09-09'
Description: 'RMIT: VPC + 2 public subnets + IGW + routes + SGs (lab-safe)'

Parameters:
  ProjectTag: { Type: String, Default: rmit }
  CidrBlock:  { Type: String, Default: 10.10.0.0/16 }

Resources:
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref CidrBlock
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags: [{Key: Project, Value: !Ref ProjectTag}]

  IGW: { Type: AWS::EC2::InternetGateway }

  VPCIGWAttach:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties: { InternetGatewayId: !Ref IGW, VpcId: !Ref VPC }

  PubSubnetA:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.10.1.0/24
      AvailabilityZone: !Select [0, !GetAZs '']
      MapPublicIpOnLaunch: true

  PubSubnetB:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.10.2.0/24
      AvailabilityZone: !Select [1, !GetAZs '']
      MapPublicIpOnLaunch: true

  PubRouteTable: { Type: AWS::EC2::RouteTable, Properties: { VpcId: !Ref VPC } }
  PubRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PubRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref IGW
  PubAssocA: { Type: AWS::EC2::SubnetRouteTableAssociation, Properties: { SubnetId: !Ref PubSubnetA, RouteTableId: !Ref PubRouteTable } }
  PubAssocB: { Type: AWS::EC2::SubnetRouteTableAssociation, Properties: { SubnetId: !Ref PubSubnetB, RouteTableId: !Ref PubRouteTable } }

  JenkinsSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Jenkins
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - { IpProtocol: tcp, FromPort: 22,   ToPort: 22,   CidrIp: 0.0.0.0/0 }
        - { IpProtocol: tcp, FromPort: 8080, ToPort: 8080, CidrIp: 0.0.0.0/0 }

  MasterSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: k3s-master
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - { IpProtocol: tcp, FromPort: 22,  ToPort: 22,  CidrIp: 0.0.0.0/0 }
        - { IpProtocol: tcp, FromPort: 80,  ToPort: 80,  CidrIp: 0.0.0.0/0 }
        - { IpProtocol: tcp, FromPort: 443, ToPort: 443, CidrIp: 0.0.0.0/0 }
        - { IpProtocol: tcp, FromPort: 6443, ToPort: 6443, SourceSecurityGroupId: !Ref JenkinsSG }
        - { IpProtocol: tcp, FromPort: 6443, ToPort: 6443, SourceSecurityGroupId: !Ref MasterSG }
        - { IpProtocol: tcp, FromPort: 6443, ToPort: 6443, SourceSecurityGroupId: !Ref WorkerSG }

  WorkerSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: k3s-workers
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - { IpProtocol: tcp, FromPort: 22, ToPort: 22, CidrIp: 0.0.0.0/0 }
        - { IpProtocol: -1, FromPort: -1, ToPort: -1, SourceSecurityGroupId: !Ref MasterSG }
        - { IpProtocol: -1, FromPort: -1, ToPort: -1, SourceSecurityGroupId: !Ref WorkerSG }

  MongoSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Mongo-only-from-cluster
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - { IpProtocol: tcp, FromPort: 27017, ToPort: 27017, SourceSecurityGroupId: !Ref MasterSG }
        - { IpProtocol: tcp, FromPort: 27017, ToPort: 27017, SourceSecurityGroupId: !Ref WorkerSG }

Outputs:
  VpcId:      { Value: !Ref VPC }
  PubSubnetA: { Value: !Ref PubSubnetA }
  PubSubnetB: { Value: !Ref PubSubnetB }
  JenkinsSG:  { Value: !Ref JenkinsSG }
  MasterSG:   { Value: !Ref MasterSG }
  WorkerSG:   { Value: !Ref WorkerSG }
  MongoSG:    { Value: !Ref MongoSG }
```

## 4) CloudFormation — Compute (5 EC2)

infra/01-compute.yaml
```yaml

AWSTemplateFormatVersion: '2010-09-09'
Description: 'RMIT: 5 EC2 instances (lab-safe, no IAM roles)'

Parameters:
  SubnetIdA:  { Type: AWS::EC2::Subnet::Id }
  SubnetIdB:  { Type: AWS::EC2::Subnet::Id }
  JenkinsSG:  { Type: AWS::EC2::SecurityGroup::Id }
  MasterSG:   { Type: AWS::EC2::SecurityGroup::Id }
  WorkerSG:   { Type: AWS::EC2::SecurityGroup::Id }
  MongoSG:    { Type: AWS::EC2::SecurityGroup::Id }
  KeyName:    { Type: AWS::EC2::KeyPair::KeyName }
  TypeJenkins:{ Type: String, Default: t3.micro }
  TypeMaster: { Type: String, Default: t3.micro }
  TypeWorker: { Type: String, Default: t3.micro }
  TypeMongo:  { Type: String, Default: t3.micro }
  UbuntuAmiParam:
    Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
    Default: /aws/service/canonical/ubuntu/server/22.04/stable/current/amd64/hvm/ebs-gp3/ami-id

Conditions:
  UseSSM: !Equals [ !Ref UbuntuAmiParam, '']  # false when default populated (SSM allowed)

Mappings:
  # Optional fallback if lab blocks SSM; fill the AMI for your region then set UBUNTU_AMI_OVERRIDE in .env
  RegionToAmi:
    ap-southeast-2:
      AMI: ami-xxxxxxxxxxxxxxxxx

Resources:
  Jenkins:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If [ UseSSM, !FindInMap [RegionToAmi, !Ref 'AWS::Region', AMI], !Ref UbuntuAmiParam ]
      InstanceType: !Ref TypeJenkins
      KeyName: !Ref KeyName
      NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          SubnetId: !Ref SubnetIdA
          GroupSet: [!Ref JenkinsSG]
      Tags: [{Key: Name, Value: jenkins}]

  Master:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If [ UseSSM, !FindInMap [RegionToAmi, !Ref 'AWS::Region', AMI], !Ref UbuntuAmiParam ]
      InstanceType: !Ref TypeMaster
      KeyName: !Ref KeyName
      NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          SubnetId: !Ref SubnetIdA
          GroupSet: [!Ref MasterSG]
      Tags: [{Key: Name, Value: k3s-master}]

  ClientWorker:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If [ UseSSM, !FindInMap [RegionToAmi, !Ref 'AWS::Region', AMI], !Ref UbuntuAmiParam ]
      InstanceType: !Ref TypeWorker
      KeyName: !Ref KeyName
      NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          SubnetId: !Ref SubnetIdB
          GroupSet: [!Ref WorkerSG]
      Tags: [{Key: Name, Value: k3s-client}]

  ServerWorker:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If [ UseSSM, !FindInMap [RegionToAmi, !Ref 'AWS::Region', AMI], !Ref UbuntuAmiParam ]
      InstanceType: !Ref TypeWorker
      KeyName: !Ref KeyName
      NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          SubnetId: !Ref SubnetIdB
          GroupSet: [!Ref WorkerSG]
      Tags: [{Key: Name, Value: k3s-server}]

  Mongo:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !If [ UseSSM, !FindInMap [RegionToAmi, !Ref 'AWS::Region', AMI], !Ref UbuntuAmiParam ]
      InstanceType: !Ref TypeMongo
      KeyName: !Ref KeyName
      NetworkInterfaces:
        - AssociatePublicIpAddress: true
          DeviceIndex: 0
          SubnetId: !Ref SubnetIdA
          GroupSet: [!Ref MongoSG]
      Tags: [{Key: Name, Value: mongo}]

Outputs:
  JenkinsIP:   { Value: !GetAtt Jenkins.PublicIp }
  MasterIP:    { Value: !GetAtt Master.PublicIp }
  ClientIP:    { Value: !GetAtt ClientWorker.PublicIp }
  ServerIP:    { Value: !GetAtt ServerWorker.PublicIp }
  MongoIPPub:  { Value: !GetAtt Mongo.PublicIp }
  JenkinsPriv: { Value: !GetAtt Jenkins.PrivateIp }
  MasterPriv:  { Value: !GetAtt Master.PrivateIp }
  ClientPriv:  { Value: !GetAtt ClientWorker.PrivateIp }
  ServerPriv:  { Value: !GetAtt ServerWorker.PrivateIp }
  MongoPriv:   { Value: !GetAtt Mongo.PrivateIp }
```

## 5) Makefile (local one-command orchestration)

Makefile
```makefile
SHELL := /bin/bash
-include .env

STACK_NET := rmit-net
STACK_EC2 := rmit-ec2

.PHONY: up down outputs inventory provision stopall

up: cfn-net cfn-ec2 outputs inventory provision

cfn-net:
	aws cloudformation deploy --stack-name $(STACK_NET) --template-file infra/00-network.yaml --region $(AWS_REGION)

cfn-ec2:
	$(eval SUBA := $(shell aws cloudformation describe-stacks --stack-name $(STACK_NET) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | select(.OutputKey=="PubSubnetA").OutputValue'))
	$(eval SUBB := $(shell aws cloudformation describe-stacks --stack-name $(STACK_NET) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | select(.OutputKey=="PubSubnetB").OutputValue'))
	$(eval JSG := $(shell aws cloudformation describe-stacks --stack-name $(STACK_NET) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | select(.OutputKey=="JenkinsSG").OutputValue'))
	$(eval MSG := $(shell aws cloudformation describe-stacks --stack-name $(STACK_NET) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | select(.OutputKey=="MasterSG").OutputValue'))
	$(eval WSG := $(shell aws cloudformation describe-stacks --stack-name $(STACK_NET) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | select(.OutputKey=="WorkerSG").OutputValue'))
	$(eval MGSG := $(shell aws cloudformation describe-stacks --stack-name $(STACK_NET) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | select(.OutputKey=="MongoSG").OutputValue'))
	aws cloudformation deploy --stack-name $(STACK_EC2) --template-file infra/01-compute.yaml --region $(AWS_REGION) \
	  --parameter-overrides SubnetIdA=$(SUBA) SubnetIdB=$(SUBB) JenkinsSG=$(JSG) MasterSG=$(MSG) WorkerSG=$(WSG) MongoSG=$(MGSG) KeyName=$(EC2_KEY_NAME) \
	  TypeJenkins=$(TYPE_JENKINS) TypeMaster=$(TYPE_MASTER) TypeWorker=$(TYPE_WORKER) TypeMongo=$(TYPE_MONGO)

outputs:
	aws cloudformation describe-stacks --stack-name $(STACK_EC2) --region $(AWS_REGION) | jq -r '.Stacks[0].Outputs[] | "\(.OutputKey)=\(.OutputValue)"' > ec2-outputs.env
	@echo "Wrote ec2-outputs.env"

inventory:
	@source ec2-outputs.env; \
	cat > ansible/inventory.ini <<EOF
[jenkins]
$${JenkinsIP} ansible_user=ubuntu ansible_ssh_private_key_file=$(EC2_KEY_NAME).pem

[k3s_master]
$${MasterIP} ansible_user=ubuntu ansible_ssh_private_key_file=$(EC2_KEY_NAME).pem

[k3s_workers]
$${ClientIP} node_label=tier=frontend ansible_user=ubuntu ansible_ssh_private_key_file=$(EC2_KEY_NAME).pem
$${ServerIP} node_label=tier=backend  ansible_user=ubuntu ansible_ssh_private_key_file=$(EC2_KEY_NAME).pem

[mongo]
$${MongoIPPub} ansible_user=ubuntu ansible_ssh_private_key_file=$(EC2_KEY_NAME).pem
EOF
	@mongo_priv=$$(grep MongoPriv ec2-outputs.env | cut -d= -f2); \
	mkdir -p ansible/group_vars; \
	cat > ansible/group_vars/all.yaml <<EOF
dockerhub_user: "$(DOCKERHUB_USER)"
dockerhub_pat:  "$(DOCKERHUB_PAT)"
github_owner:   "$(GITHUB_OWNER)"
github_repo:    "$(GITHUB_REPO)"
github_token:   "$(GITHUB_TOKEN)"
jenkins_url_hint: "$(JENKINS_URL_HINT)"
mongo_uri: "mongodb://$${mongo_priv}:27017/rmit"
jwt_secret: "prodsecret"
EOF
	@echo "Created ansible/inventory.ini and group_vars/all.yaml"

provision:
	ansible-playbook -i ansible/inventory.ini ansible/site.yaml

down:
	-aws cloudformation delete-stack --stack-name $(STACK_EC2) --region $(AWS_REGION)
	-aws cloudformation delete-stack --stack-name $(STACK_NET) --region $(AWS_REGION)

stopall:
	@source ec2-outputs.env; ids=$$(aws ec2 describe-instances --filters "Name=ip-address,Values=$${JenkinsIP},$${MasterIP},$${ClientIP},$${ServerIP},$${MongoIPPub}" --query 'Reservations[].Instances[].InstanceId' --output text --region $(AWS_REGION)); \
	aws ec2 stop-instances --instance-ids $$ids --region $(AWS_REGION)
```

## 6) Ansible — Play + Roles

ansible/site.yaml

- hosts: all
  become: yes
  roles: [common]

- hosts: mongo
  become: yes
  roles: [mongo]

- hosts: k3s_master
  become: yes
  roles: [k3s_master]

- hosts: k3s_workers
  become: yes
  roles: [k3s_worker]

- hosts: jenkins
  become: yes
  roles: [jenkins]

roles/common/tasks/main.yaml
- name: Update apt & install base
  apt:
    update_cache: yes
    name: [curl, git, unzip, docker.io]
    state: present

- name: Enable docker
  systemd: { name: docker, state: started, enabled: yes }

- name: Add ubuntu to docker group
  user: { name: ubuntu, groups: docker, append: yes }

roles/mongo/tasks/main.yaml
- name: Create mongo data dir
  file: { path: /var/lib/mongo, state: directory, owner: root, group: root, mode: '0755' }

- name: Run mongo
  docker_container:
    name: mongo-prod
    image: mongo:6
    state: started
    restart_policy: unless-stopped
    published_ports: ["27017:27017"]
    volumes:
      - /var/lib/mongo:/data/db

roles/k3s_master/tasks/main.yaml
- name: Install k3s server (Traefik enabled)
  shell: curl -sfL https://get.k3s.io | sh -s -

- name: Read node token
  slurp: { src: /var/lib/rancher/k3s/server/node-token }
  register: nodetoken_raw

- name: Get master private IP
  command: curl -s http://169.254.169.254/latest/meta-data/local-ipv4
  register: master_priv
  changed_when: false

- name: Save facts for workers and Jenkins
  set_fact:
    k3s_token: "{{ nodetoken_raw.content | b64decode | trim }}"
    master_private_ip: "{{ master_priv.stdout }}"

- name: Ensure namespace prod
  shell: kubectl create ns prod || true
  environment: { KUBECONFIG: /etc/rancher/k3s/k3s.yaml }

- name: Copy kubeconfig for later (Jenkins credential)
  copy:
    src: /etc/rancher/k3s/k3s.yaml
    dest: /home/ubuntu/kubeconfig.yaml
    owner: ubuntu
    mode: '0600'

- name: Slurp kubeconfig (base64) for JCasC
  slurp: { src: /home/ubuntu/kubeconfig.yaml }
  register: kubeconf_b64

- name: Set kubeconfig_b64 fact (for jenkins role)
  set_fact:
    kubeconfig_b64: "{{ kubeconf_b64.content }}"

roles/k3s_worker/tasks/main.yaml
- name: Join k3s agent
  shell: |
    curl -sfL https://get.k3s.io | \
    K3S_URL=https://{{ hostvars[groups['k3s_master'][0]].master_private_ip }}:6443 \
    K3S_TOKEN={{ hostvars[groups['k3s_master'][0]].k3s_token }} sh -
  args: { executable: /bin/bash }

- name: Label worker node (tier)
  shell: |
    kubectl label node $(hostname) {{ hostvars[inventory_hostname].node_label }} --overwrite
  environment: { KUBECONFIG: /etc/rancher/k3s/k3s.yaml }
  ignore_errors: yes

roles/jenkins/tasks/main.yaml
- name: Create Jenkins config dir
  file: { path: /opt/jenkins, state: directory, mode: '0755' }

- name: Write plugins.txt
  copy:
    dest: /opt/jenkins/plugins.txt
    content: |
      git
      workflow-aggregator
      blueocean
      docker-workflow
      email-ext
      configuration-as-code
      kubernetes-cli
      github
      github-branch-source
      job-dsl

- name: Render casc.yaml from template (inject secrets)
  template:
    src: "{{ playbook_dir }}/../jenkins/casc.yaml.j2"
    dest: /opt/jenkins/casc.yaml
    mode: '0644'
  vars:
    kubeconfig_b64: "{{ hostvars[groups['k3s_master'][0]].kubeconfig_b64 }}"
    dockerhub_user: "{{ dockerhub_user }}"
    dockerhub_pat:  "{{ dockerhub_pat }}"
    github_owner:   "{{ github_owner }}"
    github_repo:    "{{ github_repo }}"
    github_token:   "{{ github_token }}"
    jenkins_url_hint: "{{ jenkins_url_hint }}"

- name: Run Jenkins container
  docker_container:
    name: jenkins
    image: jenkins/jenkins:lts
    restart_policy: unless-stopped
    state: started
    published_ports: ["8080:8080","50000:50000"]
    volumes:
      - /var/jenkins_home:/var/jenkins_home
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/jenkins:/config
    env:
      CASC_JENKINS_CONFIG: /config/casc.yaml

- name: Install plugins via CLI
  shell: docker exec jenkins jenkins-plugin-cli -f /config/plugins.txt

- name: Restart Jenkins to load plugins + JCasC
  docker_container:
    name: jenkins
    restart: yes

## 7) Jenkins JCasC (auto-credentials + seed job)

jenkins/plugins.txt is already created by Ansible.
Create jenkins/casc.yaml.j2:

jenkins:
  systemMessage: "RMIT Jenkins (auto-configured)"
unclassified:
  location:
    url: "{{ jenkins_url_hint }}"
credentials:
  system:
    domainCredentials:
      - credentials:
          - usernamePassword:
              id: "dockerhub"
              description: "Docker Hub"
              username: "{{ dockerhub_user }}"
              password: "{{ dockerhub_pat }}"
              scope: GLOBAL
          - string:
              id: "github-token"
              description: "GitHub PAT for checkout and API"
              secret: "{{ github_token }}"
              scope: GLOBAL
          - file:
              id: "kubeconfig-master"
              description: "k3s kubeconfig"
              fileName: "kubeconfig.yaml"
              secretBytes: "{{ kubeconfig_b64 }}"
              scope: GLOBAL

jobs:
  - script: >
      multibranchPipelineJob('rmit-store') {
        branchSources {
          branchSource {
            source {
              github {
                id('gh')
                repoOwner('{{ github_owner }}')
                repository('{{ github_repo }}')
                credentialsId('github-token')
                configuredByUrl(false)
                repositoryUrl('https://github.com/{{ github_owner }}/{{ github_repo }}.git')
              }
            }
          }
        }
        orphanedItemStrategy {
          discardOldItems {
            daysToKeep(7)
            numToKeep(10)
          }
        }
        factory {
          workflowBranchProjectFactory {
            scriptPath('Jenkinsfile')
          }
        }
        // Poll every minute (works even if webhooks are blocked in lab)
        triggers { periodicFolderTrigger { interval('1m') } }
      }


The pipeline will use your repo’s Jenkinsfile. In that file, use the credentials IDs:

Docker Hub: dockerhub

kubeconfig file: kubeconfig-master (use withCredentials([file(credentialsId: 'kubeconfig-master', variable: 'KUBECONF')]))

## 8) Bring Everything Up
cd rmit-aws
cp .env.example .env
# edit .env to set AWS_REGION, EC2_KEY_NAME, DockerHub, GitHub vars, etc.
source .env

# Run full provisioning + bootstrap (from your laptop)
make up

# (Optional) stop all EC2s when idle to save credits)
make stopall

## 9) After Provisioning

Jenkins URL: http://<JENKINS_PUBLIC_IP>:8080
JCasC applies automatically; the multibranch job rmit-store appears and starts polling your repo.

k3s entrypoint: http://<MASTER_PUBLIC_IP>/ (Traefik on k3s master)

Mongo: private, reachable only from the cluster SGs.

Your pipeline (in the repo Jenkinsfile) should: build → test → push to Docker Hub → helm upgrade --install to prod using the kubeconfig credential.
Use node selectors to pin FE to Client node and BE to Server node.

## 10) Lab-Role Notes (workarounds)

Key pair: use an existing lab key pair name (EC2_KEY_NAME).

SSM AMI param blocked? Put a region AMI ID in UBUNTU_AMI_OVERRIDE and fill the map in 01-compute.yaml; or replace the ImageId with your AMI.

Webhooks blocked? The seed job uses periodic polling every minute—no webhooks required.

No IAM roles created: All containers run without instance profiles; Docker Hub/GitHub creds are injected via Jenkins credentials (JCasC).

Budget: all instances t3.micro. Stop them when idle (make stopall).

## 11) Minimal Jenkinsfile (example reference)

Put this in your app repo. Adjust image names and chart path.

pipeline {
  agent any
  environment {
    DH_NS     = 'your_dockerhub_user'
    FRONT_IMG = "docker.io/${DH_NS}/rmit-store-frontend"
    BACK_IMG  = "docker.io/${DH_NS}/rmit-store-backend"
    CHART_DIR = "helm/rmit-store"
  }
  stages {
    stage('Checkout'){ steps { checkout scm } }

    stage('Build Images'){
      steps {
        sh """
          docker build -t ${BACK_IMG}:${GIT_COMMIT}  ./server
          docker build -t ${FRONT_IMG}:${GIT_COMMIT} ./client
        """
      }
    }

    stage('Push'){
      steps {
        withCredentials([usernamePassword(credentialsId: 'dockerhub', usernameVariable: 'DH_USER', passwordVariable: 'DH_PASS')]){
          sh """
            echo "$DH_PASS" | docker login -u "$DH_USER" --password-stdin
            docker push ${BACK_IMG}:${GIT_COMMIT}
            docker push ${FRONT_IMG}:${GIT_COMMIT}
          """
        }
      }
    }

    stage('Deploy (blue→green)'){
      steps {
        withCredentials([file(credentialsId: 'kubeconfig-master', variable: 'KUBECONF')]){
          sh """
            export KUBECONFIG=$KUBECONF
            helm upgrade --install rmit-store ${CHART_DIR} -n prod \
              --set backend.greenImage=${BACK_IMG}:${GIT_COMMIT} \
              --set frontend.greenImage=${FRONT_IMG}:${GIT_COMMIT} \
              --set backend.activeColor=blue --set frontend.activeColor=blue
            kubectl -n prod rollout status deploy/backend-green --timeout=120s || true
            kubectl -n prod rollout status deploy/frontend-green --timeout=120s || true
            kubectl -n prod patch service backend  -p '{"spec":{"selector":{"app":"backend","activeColor":"green"}}}'
            kubectl -n prod patch service frontend -p '{"spec":{"selector":{"app":"frontend","activeColor":"green"}}}'
          """
        }
      }
    }
  }
}

## 12) Helm/K8s Secrets (created once by pipeline or Ansible)

Create Docker Hub pull secret in prod and patch default SA, or let your chart use imagePullSecrets.

Create app-secrets with:

MONGODB_URI=mongodb://<MongoPrivateIP>:27017/rmit

JWT_SECRET=prodsecret

You can do this from the pipeline before deploy, or add Ansible tasks in k3s_master to run:

kubectl -n prod create secret docker-registry dockerhub-regcred \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username="$DOCKERHUB_USER" \
  --docker-password="$DOCKERHUB_PAT" \
  --docker-email="you@example.com"

kubectl -n prod create secret generic app-secrets \
  --from-literal=MONGODB_URI="mongodb://<MongoPrivateIP>:27017/rmit" \
  --from-literal=JWT_SECRET="prodsecret" --dry-run=client -o yaml | kubectl apply -f -

## 13) Run It
# from your laptop
cd rmit-aws
source .env
make up
# push a commit to your GitHub repo; Jenkins seed job will detect/scan and run the pipeline.